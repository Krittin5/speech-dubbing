{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krittinnagar/Speech Dubbing /.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/krittinnagar/Speech Dubbing /.venv/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/krittinnagar/Speech Dubbing /.venv/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import librosa\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sounddevice as sd\n",
    "# import soundfile as sf\n",
    "# import os\n",
    "\n",
    "# # First install: pip install sounddevice\n",
    "\n",
    "# # Create folder\n",
    "# os.makedirs(\"test_audio\", exist_ok=True)\n",
    "\n",
    "# # Hindi sentences to record\n",
    "# sentences = [\n",
    "#     \"à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤®à¥‡à¤°à¤¾ à¤¨à¤¾à¤® à¤°à¤¾à¤œ à¤¹à¥ˆ\",\n",
    "#     \"à¤†à¤œ à¤®à¥Œà¤¸à¤® à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ\",\n",
    "#     \"à¤®à¥à¤à¥‡ à¤¹à¤¿à¤‚à¤¦à¥€ à¤¬à¥‹à¤²à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ\",\n",
    "#     \"à¤¯à¤¹ à¤à¤• à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤µà¤¾à¤•à¥à¤¯ à¤¹à¥ˆ\",\n",
    "#     \"à¤­à¤¾à¤°à¤¤ à¤à¤• à¤¸à¥à¤‚à¤¦à¤° à¤¦à¥‡à¤¶ à¤¹à¥ˆ\"\n",
    "# ]\n",
    "\n",
    "# print(\"Record 5 Hindi sentences (5 seconds each)\\n\")\n",
    "\n",
    "# if os.environ.get(\"SKIP_AUDIO_RECORDING\", \"0\") == \"1\":\n",
    "#     print(\"â­ï¸ Skipping interactive microphone recording in automated mode.\")\n",
    "# else:\n",
    "#     for i, sentence in enumerate(sentences):\n",
    "#         print(f\"\\nSample {i+1}: {sentence}\")\n",
    "#         input(\"Press Enter when ready to record...\")\n",
    "#         print(\"ðŸ”´ Recording... (5 seconds)\")\n",
    "        \n",
    "#         # Record\n",
    "#         audio = sd.rec(int(5 * 16000), samplerate=16000, channels=1)\n",
    "#         sd.wait()\n",
    "        \n",
    "#         # Save audio\n",
    "#         sf.write(f\"test_audio/sample_{i+1}.wav\", audio, 16000)\n",
    "        \n",
    "#         # Save text\n",
    "#         with open(f\"test_audio/sample_{i+1}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(sentence)\n",
    "        \n",
    "#         print(f\"âœ“ Saved sample_{i+1}.wav\")\n",
    "\n",
    "#     print(\"\\nâœ… Done! You now have 5 Hindi audio samples in 'test_audio/' folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krittinnagar/Speech Dubbing /.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krittinnagar/Speech Dubbing /.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Some weights of the model checkpoint at ai4bharat/indicwav2vec-hindi were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ai4bharat/indicwav2vec-hindi and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded!\n",
      "\n",
      "Found 15 files\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¤ sample_1.wav\n",
      "ðŸ“ Actual:    à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤®à¥‡à¤°à¤¾ à¤¨à¤¾à¤® à¤°à¤¾à¤œ à¤¹à¥ˆ\n",
      "ðŸ¤– Predicted: à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤®à¥‡à¤°à¤¾ à¤¨à¤¾à¤® à¤°à¤¾à¤œ à¤¹à¥ˆ\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_1_english_dubbed.wav\n",
      "ðŸ¤– Predicted: à¤®à¤¾à¤à¤‚ à¤¨à¤ˆ à¤®à¤¿à¤¶à¥à¤–à¤¿à¤‚à¤¡à¤®\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_1_tamil_voice_cloned.wav\n",
      "ðŸ¤– Predicted: à¤¨à¥à¤¯à¤¾à¤¯ à¤¸à¤‚à¤¥à¥€ à¤°à¥‡à¤‚à¤ à¥à¤¯à¤¾ à¤¡à¥‡à¤¨à¤¾\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_2.wav\n",
      "ðŸ“ Actual:    à¤†à¤œ à¤®à¥Œà¤¸à¤® à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ\n",
      "ðŸ¤– Predicted: à¤†à¤œ à¤®à¥Œà¤¸à¤® à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_2_english_dubbed.wav\n",
      "ðŸ¤– Predicted: à¤¦ à¤µà¤¾à¤¦à¤°à¤¸ à¤µà¥‡à¤¡à¥€à¤•à¤¯à¥à¤¦à¥à¤§ à¤¦à¥‡\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_2_tamil_voice_cloned.wav\n",
      "ðŸ¤– Predicted: à¤œà¤°à¤¾ à¤•à¥ˆ à¤œà¤¾à¤¯ à¤¦à¥‡à¤–à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤µà¥ƒà¤¶ à¤•à¥ à¤—à¥‹à¤¦ à¤\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_3.wav\n",
      "ðŸ“ Actual:    à¤®à¥à¤à¥‡ à¤¹à¤¿à¤‚à¤¦à¥€ à¤¬à¥‹à¤²à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ\n",
      "ðŸ¤– Predicted: à¤®à¥à¤à¥‡ à¤¹à¤¿à¤‚à¤¦à¥€ à¤¬à¥‹à¤²à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_3_english_dubbed.wav\n",
      "ðŸ¤– Predicted: à¤†à¤ˆ à¤²à¤¾à¤‡à¤• à¤¸ à¤¸à¥à¤ªà¥€à¤• à¤¹à¤¿à¤‚à¤¦à¥‚\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_3_tamil_voice_cloned.wav\n",
      "ðŸ¤– Predicted: à¤¦à¥‡à¤¶à¥à¤¯ à¤ªà¥‡à¤¦à¥‹à¤¸à¥€ à¤•à¤²à¤¾à¤¯ à¤ªà¤•à¤¤à¤¿ à¤¨à¥‡ à¤¯à¤¾ à¤¤à¥€à¤°à¤–à¥€ à¤¹à¥ˆ\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_4.wav\n",
      "ðŸ“ Actual:    à¤¯à¤¹ à¤à¤• à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤µà¤¾à¤•à¥à¤¯ à¤¹à¥ˆ\n",
      "ðŸ¤– Predicted: à¤¯à¤¹ à¤à¤• à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤µà¤¾à¤•à¥à¤¯ à¤¹à¥ˆ\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_4_english_dubbed.wav\n",
      "ðŸ¤– Predicted: à¤¦à¤¿à¤¸ à¤‡à¤œ à¤¥à¥ˆà¤¸ à¤ªà¥à¤°à¥‡à¤¸\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_4_tamil_voice_cloned.wav\n",
      "ðŸ¤– Predicted: à¤¤à¥‡à¤¸à¥à¤¥ à¤ªà¥à¤°à¤¾à¤¸\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_5.wav\n",
      "ðŸ“ Actual:    à¤­à¤¾à¤°à¤¤ à¤à¤• à¤¸à¥à¤‚à¤¦à¤° à¤¦à¥‡à¤¶ à¤¹à¥ˆ\n",
      "ðŸ¤– Predicted: à¤­à¤¾à¤°à¤¤ à¤à¤• à¤¸à¥à¤‚à¤¦à¤° à¤¦à¥‡à¤¶ à¤¹à¥ˆ\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_5_english_dubbed.wav\n",
      "ðŸ¤– Predicted: à¤‡à¤‚à¤¦à¤¿à¤¯à¤¾ à¤‡à¤¸ à¤¬à¥à¤¯à¥‚à¤°à¥‹à¤«à¥‹ à¤–à¤¾à¤¨ à¤¶à¥à¤\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¤ sample_5_tamil_voice_cloned.wav\n",
      "ðŸ¤– Predicted: à¤¾à¤¨à¤¾à¤œà¥‡à¤¨à¤šà¤¦à¥à¤°à¤¾à¤ à¤ªà¤¿à¤¶à¤£à¤¦à¤§ à¤«à¤¯à¥‚à¤¦à¤¿à¤¯à¤¾ à¤›à¥‹à¤­à¥à¤°à¥ˆà¤š à¤·à¤¿à¤¯à¤¾ à¤ªà¥‹à¤¡à¤¾à¤°à¥‡\n",
      "------------------------------------------------------------\n",
      "\n",
      "âœ… Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Loading model...\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"ai4bharat/indicwav2vec-hindi\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"ai4bharat/indicwav2vec-hindi\")\n",
    "model.eval()\n",
    "print(\"âœ… Model loaded!\\n\")\n",
    "\n",
    "audio_files = sorted(Path(\"test_audio\").glob(\"*.wav\"))\n",
    "print(f\"Found {len(audio_files)} files\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for audio_path in audio_files:\n",
    "    print(f\"\\nðŸŽ¤ {audio_path.name}\")\n",
    "    \n",
    "    # Load audio with torchaudio\n",
    "    waveform, sample_rate = torchaudio.load(str(audio_path))\n",
    "    \n",
    "    # Resample to 16kHz if needed\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Squeeze to 1D\n",
    "    waveform = waveform.squeeze().numpy()\n",
    "    \n",
    "    # Process\n",
    "    input_values = processor(waveform, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    \n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "    \n",
    "    # Show results\n",
    "    txt_file = audio_path.with_suffix('.txt')\n",
    "    if txt_file.exists():\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            actual = f.read().strip()\n",
    "        print(f\"ðŸ“ Actual:    {actual}\")\n",
    "        print(f\"ðŸ¤– Predicted: {transcription}\")\n",
    "    else:\n",
    "        print(f\"ðŸ¤– Predicted: {transcription}\")\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "\n",
    "print(\"\\nâœ… Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Hindi STT model...\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ai4bharat/indicwav2vec-hindi were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ai4bharat/indicwav2vec-hindi and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… STT loaded!\n",
      "\n",
      "Loading Hindi to English translation model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krittinnagar/Speech Dubbing /.venv/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Translation model loaded!\n",
      "\n",
      "============================================================\n",
      "AUDIO -> HINDI -> ENGLISH PIPELINE\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¤ sample_1.wav\n",
      "   Original: à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤®à¥‡à¤°à¤¾ à¤¨à¤¾à¤® à¤°à¤¾à¤œ à¤¹à¥ˆ\n",
      "   Hindi:    à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤®à¥‡à¤°à¤¾ à¤¨à¤¾à¤® à¤°à¤¾à¤œ à¤¹à¥ˆ\n",
      "   English:  Hello my name is the Kingdom.\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_1_english_dubbed.wav\n",
      "   Hindi:    à¤®à¤¾à¤à¤‚ à¤¨à¤ˆ à¤®à¤¿à¤¶à¥à¤–à¤¿à¤‚à¤¡à¤®\n",
      "   English:  Jess New Mundineum\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_1_tamil_voice_cloned.wav\n",
      "   Hindi:    à¤¨à¥à¤¯à¤¾à¤¯ à¤¸à¤‚à¤¥à¥€ à¤°à¥‡à¤‚à¤ à¥à¤¯à¤¾ à¤¡à¥‡à¤¨à¤¾\n",
      "   English:  Judgment Day Afflictive Despera\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_2.wav\n",
      "   Original: à¤†à¤œ à¤®à¥Œà¤¸à¤® à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ\n",
      "   Hindi:    à¤†à¤œ à¤®à¥Œà¤¸à¤® à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ\n",
      "   English:  The Weather Is Very Good Today\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_2_english_dubbed.wav\n",
      "   Hindi:    à¤¦ à¤µà¤¾à¤¦à¤°à¤¸ à¤µà¥‡à¤¡à¥€à¤•à¤¯à¥à¤¦à¥à¤§ à¤¦à¥‡\n",
      "   English:  Give The Vedus Wedik War.\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_2_tamil_voice_cloned.wav\n",
      "   Hindi:    à¤œà¤°à¤¾ à¤•à¥ˆ à¤œà¤¾à¤¯ à¤¦à¥‡à¤–à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤µà¥ƒà¤¶ à¤•à¥ à¤—à¥‹à¤¦ à¤\n",
      "   English:  Let's see just how I feel.\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_3.wav\n",
      "   Original: à¤®à¥à¤à¥‡ à¤¹à¤¿à¤‚à¤¦à¥€ à¤¬à¥‹à¤²à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ\n",
      "   Hindi:    à¤®à¥à¤à¥‡ à¤¹à¤¿à¤‚à¤¦à¥€ à¤¬à¥‹à¤²à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ\n",
      "   English:  I prefer to speak\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_3_english_dubbed.wav\n",
      "   Hindi:    à¤†à¤ˆ à¤²à¤¾à¤‡à¤• à¤¸ à¤¸à¥à¤ªà¥€à¤• à¤¹à¤¿à¤‚à¤¦à¥‚\n",
      "   English:  ILCK S SpicK Hindu\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_3_tamil_voice_cloned.wav\n",
      "   Hindi:    à¤¦à¥‡à¤¶à¥à¤¯ à¤ªà¥‡à¤¦à¥‹à¤¸à¥€ à¤•à¤²à¤¾à¤¯ à¤ªà¤•à¤¤à¤¿ à¤¨à¥‡ à¤¯à¤¾ à¤¤à¥€à¤°à¤–à¥€ à¤¹à¥ˆ\n",
      "   English:  The country's Pedicicisical art is either an arrow or an arrow\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_4.wav\n",
      "   Original: à¤¯à¤¹ à¤à¤• à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤µà¤¾à¤•à¥à¤¯ à¤¹à¥ˆ\n",
      "   Hindi:    à¤¯à¤¹ à¤à¤• à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤µà¤¾à¤•à¥à¤¯ à¤¹à¥ˆ\n",
      "   English:  This is a test sentence\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_4_english_dubbed.wav\n",
      "   Hindi:    à¤¦à¤¿à¤¸ à¤‡à¤œ à¤¥à¥ˆà¤¸ à¤ªà¥à¤°à¥‡à¤¸\n",
      "   English:  D-Bus Press\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_4_tamil_voice_cloned.wav\n",
      "   Hindi:    à¤¤à¥‡à¤¸à¥à¤¥ à¤ªà¥à¤°à¤¾à¤¸\n",
      "   English:  Teres\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_5.wav\n",
      "   Original: à¤­à¤¾à¤°à¤¤ à¤à¤• à¤¸à¥à¤‚à¤¦à¤° à¤¦à¥‡à¤¶ à¤¹à¥ˆ\n",
      "   Hindi:    à¤­à¤¾à¤°à¤¤ à¤à¤• à¤¸à¥à¤‚à¤¦à¤° à¤¦à¥‡à¤¶ à¤¹à¥ˆ\n",
      "   English:  India is a beautiful country\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_5_english_dubbed.wav\n",
      "   Hindi:    à¤‡à¤‚à¤¦à¤¿à¤¯à¤¾ à¤‡à¤¸ à¤¬à¥à¤¯à¥‚à¤°à¥‹à¤«à¥‹ à¤–à¤¾à¤¨ à¤¶à¥à¤\n",
      "   English:  Investigation Investitut Shuk\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_5_tamil_voice_cloned.wav\n",
      "   Hindi:    à¤¾à¤¨à¤¾à¤œà¥‡à¤¨à¤šà¤¦à¥à¤°à¤¾à¤ à¤ªà¤¿à¤¶à¤£à¤¦à¤§ à¤«à¤¯à¥‚à¤¦à¤¿à¤¯à¤¾ à¤›à¥‹à¤­à¥à¤°à¥ˆà¤š à¤·à¤¿à¤¯à¤¾ à¤ªà¥‹à¤¡à¤¾à¤°à¥‡\n",
      "   English:  Jugenid Fupopoporous Posiana Posche Â· Global Voices\n",
      "------------------------------------------------------------\n",
      "\n",
      "âœ… Pipeline complete!\n",
      "\n",
      "Next: We'll translate English -> Telugu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "\n",
    "# Load STT model\n",
    "print(\"Loading Hindi STT model...\")\n",
    "stt_processor = Wav2Vec2Processor.from_pretrained(\"ai4bharat/indicwav2vec-hindi\")\n",
    "stt_model = Wav2Vec2ForCTC.from_pretrained(\"ai4bharat/indicwav2vec-hindi\")\n",
    "stt_model.eval()\n",
    "print(\"âœ… STT loaded!\")\n",
    "\n",
    "# Load Translation model (Hindi -> English)\n",
    "print(\"\\nLoading Hindi to English translation model...\")\n",
    "translation_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-hi-en\")\n",
    "translation_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-hi-en\")\n",
    "translation_model.eval()\n",
    "print(\"âœ… Translation model loaded!\\n\")\n",
    "\n",
    "def translate_hindi_to_english(hindi_text):\n",
    "    \"\"\"Translate Hindi to English\"\"\"\n",
    "    inputs = translation_tokenizer(hindi_text, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        translated = translation_model.generate(**inputs, max_length=512)\n",
    "    return translation_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "# Process audio files\n",
    "audio_files = sorted(Path(\"test_audio\").glob(\"*.wav\"))\n",
    "print(\"=\"*60)\n",
    "print(\"AUDIO -> HINDI -> ENGLISH PIPELINE\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for audio_path in audio_files:\n",
    "    print(f\"ðŸŽ¤ {audio_path.name}\")\n",
    "    \n",
    "    # Step 1: Audio -> Hindi\n",
    "    waveform, sample_rate = torchaudio.load(str(audio_path))\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    waveform = waveform.squeeze().numpy()\n",
    "    \n",
    "    input_values = stt_processor(waveform, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "    with torch.no_grad():\n",
    "        logits = stt_model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    hindi_text = stt_processor.decode(predicted_ids[0])\n",
    "    \n",
    "    # Step 2: Hindi -> English\n",
    "    english_text = translate_hindi_to_english(hindi_text)\n",
    "    \n",
    "    # Show results\n",
    "    txt_file = audio_path.with_suffix('.txt')\n",
    "    if txt_file.exists():\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            original = f.read().strip()\n",
    "        print(f\"   Original: {original}\")\n",
    "    \n",
    "    print(f\"   Hindi:    {hindi_text}\")\n",
    "    print(f\"   English:  {english_text}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "print(\"\\nâœ… Pipeline complete!\")\n",
    "print(\"\\nNext: We'll translate English -> Telugu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Hindi STT model...\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ai4bharat/indicwav2vec-hindi were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ai4bharat/indicwav2vec-hindi and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Results saved to: hindi_to_tamil_results.csv\n",
      "\n",
      "============================================================\n",
      "PIPELINE SUMMARY\n",
      "============================================================\n",
      "âœ… Speech-to-Text: Hindi Audio -> Hindi Text\n",
      "âœ… Translation 1:   Hindi Text -> English Text\n",
      "âœ… Translation 2:   English Text -> Tamil Text\n",
      "\n",
      "Next step: Tamil Text -> Tamil Speech (TTS)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Loading Hindi STT model...\")\n",
    "stt_processor = Wav2Vec2Processor.from_pretrained(\"ai4bharat/indicwav2vec-hindi\")\n",
    "stt_model = Wav2Vec2ForCTC.from_pretrained(\"ai4bharat/indicwav2vec-hindi\")\n",
    "stt_model.eval()\n",
    "\n",
    "translation_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "translation_tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "def translate_hindi_to_english(hindi_text):\n",
    "    \"\"\"Translate Hindi to English\"\"\"\n",
    "    translation_tokenizer.src_lang = \"hi\"  # Hindi source\n",
    "    encoded = translation_tokenizer(hindi_text, return_tensors=\"pt\")\n",
    "    generated_tokens = translation_model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=translation_tokenizer.get_lang_id(\"en\")  # English\n",
    "    )\n",
    "    return translation_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "def translate_english_to_tamil(english_text):\n",
    "    \"\"\"Translate English to Tamil\"\"\"\n",
    "    translation_tokenizer.src_lang = \"en\"  # English source\n",
    "    encoded = translation_tokenizer(english_text, return_tensors=\"pt\")\n",
    "    generated_tokens = translation_model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=translation_tokenizer.get_lang_id(\"ta\")  # Tamil target\n",
    "    )\n",
    "    return translation_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "# Process audio files\n",
    "audio_files = sorted(Path(\"test_audio\").glob(\"*.wav\"))\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for audio_path in audio_files:\n",
    "    \n",
    "    waveform, sample_rate = torchaudio.load(str(audio_path))\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    waveform = waveform.squeeze().numpy()\n",
    "    \n",
    "    input_values = stt_processor(waveform, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "    with torch.no_grad():\n",
    "        logits = stt_model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    hindi_text = stt_processor.decode(predicted_ids[0])\n",
    "    \n",
    "    english_text = translate_hindi_to_english(hindi_text)\n",
    "    \n",
    "    tamil_text = translate_english_to_tamil(english_text)\n",
    "    \n",
    "    txt_file = audio_path.with_suffix('.txt')\n",
    "    if txt_file.exists():\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            original = f.read().strip()\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'file': audio_path.name,\n",
    "        'original': original if txt_file.exists() else '',\n",
    "        'hindi': hindi_text,\n",
    "        'english': english_text,\n",
    "        'tamil': tamil_text\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"hindi_to_tamil_results.csv\", index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nðŸ’¾ Results saved to: hindi_to_tamil_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… Speech-to-Text: Hindi Audio -> Hindi Text\")\n",
    "print(\"âœ… Translation 1:   Hindi Text -> English Text\")\n",
    "print(\"âœ… Translation 2:   English Text -> Tamil Text\")\n",
    "print(\"\\nNext step: Tamil Text -> Tamil Speech (TTS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krittinnagar/Speech Dubbing /.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/krittinnagar/Speech Dubbing /.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Some weights of the model checkpoint at ai4bharat/indicwav2vec-hindi were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ai4bharat/indicwav2vec-hindi and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/krittinnagar/Speech Dubbing /.venv/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤ sample_1.wav\n",
      "   Original:  à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤®à¥‡à¤°à¤¾ à¤¨à¤¾à¤® à¤°à¤¾à¤œ à¤¹à¥ˆ\n",
      "   Hindi:     à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤®à¥‡à¤°à¤¾ à¤¨à¤¾à¤® à¤°à¤¾à¤œ à¤¹à¥ˆ\n",
      "   ðŸ“Š Context: Positive (47.7%)\n",
      "      Positive: 47.7%, Neutral: 41.7%, Negative: 10.7%\n",
      "   English:   My name is Kingdom.\n",
      "   Tamil:     à®Žà®©à¯ à®ªà¯†à®¯à®°à¯ à®…à®°à®šà®¾à®™à¯à®•à®®à¯.\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_1_english_dubbed.wav\n",
      "   Hindi:     à¤®à¤¾à¤à¤‚ à¤¨à¤ˆ à¤®à¤¿à¤¶à¥à¤–à¤¿à¤‚à¤¡à¤®\n",
      "   ðŸ“Š Context: Positive (43.2%)\n",
      "      Positive: 43.2%, Neutral: 37.4%, Negative: 19.3%\n",
      "   English:   The New Mishikandam\n",
      "   Tamil:     à®ªà¯à®¤à®¿à®¯ Mishikandam\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_1_tamil_voice_cloned.wav\n",
      "   Hindi:     à¤¨à¥à¤¯à¤¾à¤¯ à¤¸à¤‚à¤¥à¥€ à¤°à¥‡à¤‚à¤ à¥à¤¯à¤¾ à¤¡à¥‡à¤¨à¤¾\n",
      "   ðŸ“Š Context: Neutral (56.0%)\n",
      "      Positive: 15.6%, Neutral: 56.0%, Negative: 28.4%\n",
      "   English:   Judgment by Danny.\n",
      "   Tamil:     à®µà®´à®•à¯à®•à®±à®¿à®žà®°à¯ à®Ÿà®¾à®©à¯\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_2.wav\n",
      "   Original:  à¤†à¤œ à¤®à¥Œà¤¸à¤® à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ\n",
      "   Hindi:     à¤†à¤œ à¤®à¥Œà¤¸à¤® à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ\n",
      "   ðŸ“Š Context: Positive (81.5%)\n",
      "      Positive: 81.5%, Neutral: 11.8%, Negative: 6.7%\n",
      "   English:   The weather is very good today.\n",
      "   Tamil:     à®‡à®©à¯à®±à¯ à®µà®¾à®©à®¿à®²à¯ˆ à®®à®¿à®•à®µà¯à®®à¯ à®¨à®²à¯à®²à®¤à¯.\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_2_english_dubbed.wav\n",
      "   Hindi:     à¤¦ à¤µà¤¾à¤¦à¤°à¤¸ à¤µà¥‡à¤¡à¥€à¤•à¤¯à¥à¤¦à¥à¤§ à¤¦à¥‡\n",
      "   ðŸ“Š Context: Neutral (40.4%)\n",
      "      Positive: 24.1%, Neutral: 40.4%, Negative: 35.5%\n",
      "   English:   Give the wicked war.\n",
      "   Tamil:     à®¤à¯€à®¯ à®ªà¯‹à®°à¯ à®•à¯Šà®Ÿà¯.\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_2_tamil_voice_cloned.wav\n",
      "   Hindi:     à¤œà¤°à¤¾ à¤•à¥ˆ à¤œà¤¾à¤¯ à¤¦à¥‡à¤–à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤µà¥ƒà¤¶ à¤•à¥ à¤—à¥‹à¤¦ à¤\n",
      "   ðŸ“Š Context: Neutral (40.1%)\n",
      "      Positive: 29.6%, Neutral: 40.1%, Negative: 30.3%\n",
      "   English:   I look at it, I see it, I see it.\n",
      "   Tamil:     à®¨à®¾à®©à¯ à®ªà®¾à®°à¯à®•à¯à®•à®¿à®±à¯‡à®©à¯, à®¨à®¾à®©à¯ à®ªà®¾à®°à¯à®•à¯à®•à®¿à®±à¯‡à®©à¯, à®¨à®¾à®©à¯ à®ªà®¾à®°à¯à®•à¯à®•à®¿à®±à¯‡à®©à¯\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_3.wav\n",
      "   Original:  à¤®à¥à¤à¥‡ à¤¹à¤¿à¤‚à¤¦à¥€ à¤¬à¥‹à¤²à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ\n",
      "   Hindi:     à¤®à¥à¤à¥‡ à¤¹à¤¿à¤‚à¤¦à¥€ à¤¬à¥‹à¤²à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ\n",
      "   ðŸ“Š Context: Positive (49.6%)\n",
      "      Positive: 49.6%, Neutral: 35.9%, Negative: 14.5%\n",
      "   English:   I like to speak Hindu.\n",
      "   Tamil:     à®¹à¯ˆà®¤à®°à®¾à®ªà®¾à®¤à¯à®¤à®¿à®²à¯ à®ªà¯‡à®š à®µà®¿à®°à¯à®®à¯à®ªà¯à®•à®¿à®±à¯‡à®©à¯.\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_3_english_dubbed.wav\n",
      "   Hindi:     à¤†à¤ˆ à¤²à¤¾à¤‡à¤• à¤¸ à¤¸à¥à¤ªà¥€à¤• à¤¹à¤¿à¤‚à¤¦à¥‚\n",
      "   ðŸ“Š Context: Neutral (73.4%)\n",
      "      Positive: 14.7%, Neutral: 73.4%, Negative: 11.8%\n",
      "   English:   I LIKE S SPEC HINDU\n",
      "   Tamil:     à®¨à®¾à®©à¯ à®¹à¯ˆà®¤à®°à®¾à®ªà®¾à®¤à¯à®¤à¯ˆ à®¨à¯‡à®šà®¿à®•à¯à®•à®¿à®±à¯‡à®©à¯\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_3_tamil_voice_cloned.wav\n",
      "   Hindi:     à¤¦à¥‡à¤¶à¥à¤¯ à¤ªà¥‡à¤¦à¥‹à¤¸à¥€ à¤•à¤²à¤¾à¤¯ à¤ªà¤•à¤¤à¤¿ à¤¨à¥‡ à¤¯à¤¾ à¤¤à¥€à¤°à¤–à¥€ à¤¹à¥ˆ\n",
      "   ðŸ“Š Context: Negative (50.3%)\n",
      "      Positive: 16.7%, Neutral: 33.0%, Negative: 50.3%\n",
      "   English:   The country's pedotic art has or is thirsty.\n",
      "   Tamil:     à®†à®™à¯à®•à®¿à®²à®¤à¯à®¤à®¿à®²à¯ à®‡à®¤à¯ˆ Single Orgasm, Multiple Orgasm à®Žà®©à¯à®±à¯à®®à¯ à®•à¯‚à®±à¯à®•à®¿à®±à®¾à®°à¯à®•à®³à¯.\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_4.wav\n",
      "   Original:  à¤¯à¤¹ à¤à¤• à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤µà¤¾à¤•à¥à¤¯ à¤¹à¥ˆ\n",
      "   Hindi:     à¤¯à¤¹ à¤à¤• à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤µà¤¾à¤•à¥à¤¯ à¤¹à¥ˆ\n",
      "   ðŸ“Š Context: Neutral (72.7%)\n",
      "      Positive: 10.3%, Neutral: 72.7%, Negative: 17.0%\n",
      "   English:   This is a test phrase.\n",
      "   Tamil:     à®‡à®¤à¯ à®’à®°à¯ test phrase.\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_4_english_dubbed.wav\n",
      "   Hindi:     à¤¦à¤¿à¤¸ à¤‡à¤œ à¤¥à¥ˆà¤¸ à¤ªà¥à¤°à¥‡à¤¸\n",
      "   ðŸ“Š Context: Neutral (56.9%)\n",
      "      Positive: 34.7%, Neutral: 56.9%, Negative: 8.4%\n",
      "   English:   It is the press.\n",
      "   Tamil:     à®…à®¤à¯ à®¤à®¾à®©à¯ Press.\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_4_tamil_voice_cloned.wav\n",
      "   Hindi:     à¤¤à¥‡à¤¸à¥à¤¥ à¤ªà¥à¤°à¤¾à¤¸\n",
      "   ðŸ“Š Context: Neutral (39.0%)\n",
      "      Positive: 27.3%, Neutral: 39.0%, Negative: 33.8%\n",
      "   English:   The fastest\n",
      "   Tamil:     à®®à®¿à®• à®µà¯‡à®•à®®à®¾à®•\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_5.wav\n",
      "   Original:  à¤­à¤¾à¤°à¤¤ à¤à¤• à¤¸à¥à¤‚à¤¦à¤° à¤¦à¥‡à¤¶ à¤¹à¥ˆ\n",
      "   Hindi:     à¤­à¤¾à¤°à¤¤ à¤à¤• à¤¸à¥à¤‚à¤¦à¤° à¤¦à¥‡à¤¶ à¤¹à¥ˆ\n",
      "   ðŸ“Š Context: Positive (90.1%)\n",
      "      Positive: 90.1%, Neutral: 6.4%, Negative: 3.4%\n",
      "   English:   India is a beautiful country.\n",
      "   Tamil:     à®‡à®¨à¯à®¤à®¿à®¯à®¾ à®’à®°à¯ à®…à®´à®•à®¾à®© à®¨à®¾à®Ÿà¯.\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_5_english_dubbed.wav\n",
      "   Hindi:     à¤‡à¤‚à¤¦à¤¿à¤¯à¤¾ à¤‡à¤¸ à¤¬à¥à¤¯à¥‚à¤°à¥‹à¤«à¥‹ à¤–à¤¾à¤¨ à¤¶à¥à¤\n",
      "   ðŸ“Š Context: Neutral (45.3%)\n",
      "      Positive: 34.4%, Neutral: 45.3%, Negative: 20.3%\n",
      "   English:   India is this buurophone.\n",
      "   Tamil:     à®‡à®¨à¯à®¨à®¿à®±à¯à®µà®©à®¤à¯à®¤à®¿à®©à¯ à®ªà¯†à®¯à®°à¯ Buurophone.\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¤ sample_5_tamil_voice_cloned.wav\n",
      "   Hindi:     à¤¾à¤¨à¤¾à¤œà¥‡à¤¨à¤šà¤¦à¥à¤°à¤¾à¤ à¤ªà¤¿à¤¶à¤£à¤¦à¤§ à¤«à¤¯à¥‚à¤¦à¤¿à¤¯à¤¾ à¤›à¥‹à¤­à¥à¤°à¥ˆà¤š à¤·à¤¿à¤¯à¤¾ à¤ªà¥‹à¤¡à¤¾à¤°à¥‡\n",
      "   ðŸ“Š Context: Negative (47.5%)\n",
      "      Positive: 18.5%, Neutral: 34.0%, Negative: 47.5%\n",
      "   English:   The wicked wicked wicked wicked wicked\n",
      "   Tamil:     à®¤à¯€à®ªà®¾à®µà®³à®¿ à®¤à¯€à®ªà®¾à®µà®³à®¿ à®¤à¯€à®ªà®¾à®µà®³à®¿ à®¤à¯€à®ªà®¾à®µà®³à®¿ à®¤à¯€à®ªà®¾à®µà®³à®¿\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, AutoModel,\n",
    "    Wav2Vec2ForCTC, Wav2Vec2Processor,\n",
    "    M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    ")\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "stt_processor = Wav2Vec2Processor.from_pretrained(\"ai4bharat/indicwav2vec-hindi\")\n",
    "stt_model = Wav2Vec2ForCTC.from_pretrained(\"ai4bharat/indicwav2vec-hindi\")\n",
    "stt_model.eval()\n",
    "\n",
    "# IndicBERT for context\n",
    "indicbert_tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBERTv2-MLM-only\")\n",
    "indicbert_model = AutoModel.from_pretrained(\"ai4bharat/IndicBERTv2-MLM-only\")\n",
    "indicbert_model.eval()\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "sentiment_model.eval()\n",
    "\n",
    "# Translation\n",
    "translation_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "translation_tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "\n",
    "# Context analysis function\n",
    "def analyze_context(text):\n",
    "    \"\"\"Analyze sentiment and context\"\"\"\n",
    "    inputs = sentiment_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = sentiment_model(**inputs)\n",
    "        scores = outputs.logits[0].softmax(dim=0).numpy()\n",
    "    \n",
    "    labels = ['Negative', 'Neutral', 'Positive']\n",
    "    sentiment = labels[np.argmax(scores)]\n",
    "    confidence = float(np.max(scores))\n",
    "    \n",
    "    # Get IndicBERT embeddings\n",
    "    bert_inputs = indicbert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        bert_outputs = indicbert_model(**bert_inputs)\n",
    "    \n",
    "    return {\n",
    "        'sentiment': sentiment,\n",
    "        'confidence': confidence,\n",
    "        'scores': {label: float(score) for label, score in zip(labels, scores)}\n",
    "    }\n",
    "\n",
    "# Translation functions\n",
    "def translate(text, src_lang, tgt_lang):\n",
    "    translation_tokenizer.src_lang = src_lang\n",
    "    encoded = translation_tokenizer(text, return_tensors=\"pt\")\n",
    "    generated = translation_model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=translation_tokenizer.get_lang_id(tgt_lang)\n",
    "    )\n",
    "    return translation_tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "\n",
    "# Process audio files\n",
    "\n",
    "results = []\n",
    "\n",
    "for audio_path in audio_files:\n",
    "    print(f\"ðŸŽ¤ {audio_path.name}\")\n",
    "    \n",
    "    # Step 1: Audio -> Hindi\n",
    "    waveform, sr = torchaudio.load(str(audio_path))\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    input_values = stt_processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "    with torch.no_grad():\n",
    "        logits = stt_model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    hindi_text = stt_processor.decode(predicted_ids[0])\n",
    "    \n",
    "    # Step 2: Context Analysis\n",
    "    context = analyze_context(hindi_text)\n",
    "    \n",
    "    # Step 3: Translate\n",
    "    english_text = translate(hindi_text, \"hi\", \"en\")\n",
    "    tamil_text = translate(english_text, \"en\", \"ta\")\n",
    "    \n",
    "    # Show results\n",
    "    txt_file = audio_path.with_suffix('.txt')\n",
    "    if txt_file.exists():\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            print(f\"   Original:  {f.read().strip()}\")\n",
    "    \n",
    "    print(f\"   Hindi:     {hindi_text}\")\n",
    "    print(f\"   ðŸ“Š Context: {context['sentiment']} ({context['confidence']:.1%})\")\n",
    "    print(f\"      Positive: {context['scores']['Positive']:.1%}, \"\n",
    "          f\"Neutral: {context['scores']['Neutral']:.1%}, \"\n",
    "          f\"Negative: {context['scores']['Negative']:.1%}\")\n",
    "    print(f\"   English:   {english_text}\")\n",
    "    print(f\"   Tamil:     {tamil_text}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    results.append({\n",
    "        'file': audio_path.name,\n",
    "        'hindi': hindi_text,\n",
    "        'sentiment': context['sentiment'],\n",
    "        'confidence': context['confidence'],\n",
    "        'english': english_text,\n",
    "        'tamil': tamil_text\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"context_analysis_results.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krittinnagar/Speech Dubbing /.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/mms-tts-tam were not used when initializing VitsModel: ['flow.flows.0.wavenet.res_skip_layers.0.weight_g', 'posterior_encoder.wavenet.in_layers.7.weight_v', 'flow.flows.3.wavenet.res_skip_layers.0.weight_g', 'flow.flows.1.wavenet.in_layers.0.weight_v', 'flow.flows.3.wavenet.in_layers.0.weight_v', 'posterior_encoder.wavenet.res_skip_layers.0.weight_v', 'flow.flows.0.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.res_skip_layers.5.weight_g', 'posterior_encoder.wavenet.in_layers.1.weight_g', 'flow.flows.1.wavenet.res_skip_layers.0.weight_v', 'flow.flows.0.wavenet.in_layers.2.weight_g', 'flow.flows.2.wavenet.res_skip_layers.2.weight_g', 'flow.flows.2.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.12.weight_g', 'flow.flows.3.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.res_skip_layers.8.weight_g', 'posterior_encoder.wavenet.res_skip_layers.14.weight_v', 'flow.flows.1.wavenet.in_layers.0.weight_g', 'flow.flows.0.wavenet.in_layers.0.weight_g', 'flow.flows.3.wavenet.res_skip_layers.0.weight_v', 'flow.flows.0.wavenet.in_layers.1.weight_g', 'flow.flows.2.wavenet.in_layers.3.weight_v', 'posterior_encoder.wavenet.res_skip_layers.13.weight_g', 'flow.flows.3.wavenet.res_skip_layers.1.weight_g', 'flow.flows.2.wavenet.in_layers.2.weight_v', 'posterior_encoder.wavenet.res_skip_layers.3.weight_v', 'flow.flows.1.wavenet.in_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.14.weight_v', 'flow.flows.2.wavenet.res_skip_layers.3.weight_v', 'flow.flows.0.wavenet.res_skip_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.3.weight_g', 'flow.flows.1.wavenet.in_layers.1.weight_g', 'flow.flows.1.wavenet.in_layers.2.weight_g', 'flow.flows.3.wavenet.in_layers.1.weight_g', 'flow.flows.1.wavenet.res_skip_layers.0.weight_g', 'posterior_encoder.wavenet.in_layers.0.weight_v', 'posterior_encoder.wavenet.res_skip_layers.9.weight_g', 'posterior_encoder.wavenet.in_layers.9.weight_v', 'flow.flows.2.wavenet.in_layers.0.weight_g', 'posterior_encoder.wavenet.res_skip_layers.1.weight_g', 'posterior_encoder.wavenet.in_layers.6.weight_v', 'posterior_encoder.wavenet.res_skip_layers.15.weight_v', 'flow.flows.3.wavenet.res_skip_layers.3.weight_v', 'flow.flows.0.wavenet.in_layers.0.weight_v', 'flow.flows.1.wavenet.res_skip_layers.2.weight_g', 'posterior_encoder.wavenet.in_layers.14.weight_g', 'posterior_encoder.wavenet.res_skip_layers.4.weight_g', 'posterior_encoder.wavenet.in_layers.11.weight_v', 'flow.flows.3.wavenet.res_skip_layers.2.weight_g', 'posterior_encoder.wavenet.res_skip_layers.10.weight_g', 'flow.flows.1.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.5.weight_v', 'posterior_encoder.wavenet.in_layers.8.weight_g', 'posterior_encoder.wavenet.res_skip_layers.10.weight_v', 'posterior_encoder.wavenet.in_layers.4.weight_v', 'flow.flows.2.wavenet.res_skip_layers.0.weight_g', 'posterior_encoder.wavenet.res_skip_layers.7.weight_g', 'posterior_encoder.wavenet.in_layers.13.weight_v', 'flow.flows.1.wavenet.in_layers.3.weight_g', 'posterior_encoder.wavenet.in_layers.13.weight_g', 'posterior_encoder.wavenet.res_skip_layers.8.weight_v', 'posterior_encoder.wavenet.in_layers.15.weight_g', 'flow.flows.1.wavenet.in_layers.1.weight_v', 'flow.flows.2.wavenet.in_layers.3.weight_g', 'posterior_encoder.wavenet.in_layers.15.weight_v', 'flow.flows.3.wavenet.in_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.0.weight_g', 'flow.flows.2.wavenet.res_skip_layers.3.weight_g', 'posterior_encoder.wavenet.in_layers.10.weight_v', 'posterior_encoder.wavenet.res_skip_layers.7.weight_v', 'posterior_encoder.wavenet.res_skip_layers.2.weight_g', 'posterior_encoder.wavenet.res_skip_layers.12.weight_v', 'posterior_encoder.wavenet.res_skip_layers.12.weight_g', 'posterior_encoder.wavenet.res_skip_layers.0.weight_g', 'posterior_encoder.wavenet.in_layers.8.weight_v', 'flow.flows.3.wavenet.res_skip_layers.3.weight_g', 'posterior_encoder.wavenet.res_skip_layers.15.weight_g', 'posterior_encoder.wavenet.in_layers.6.weight_g', 'posterior_encoder.wavenet.res_skip_layers.6.weight_v', 'posterior_encoder.wavenet.res_skip_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.2.weight_g', 'flow.flows.2.wavenet.res_skip_layers.0.weight_v', 'flow.flows.0.wavenet.res_skip_layers.2.weight_g', 'posterior_encoder.wavenet.in_layers.9.weight_g', 'posterior_encoder.wavenet.res_skip_layers.9.weight_v', 'flow.flows.3.wavenet.in_layers.1.weight_v', 'flow.flows.2.wavenet.res_skip_layers.1.weight_g', 'flow.flows.2.wavenet.res_skip_layers.1.weight_v', 'flow.flows.0.wavenet.res_skip_layers.3.weight_v', 'flow.flows.0.wavenet.in_layers.1.weight_v', 'posterior_encoder.wavenet.res_skip_layers.6.weight_g', 'flow.flows.1.wavenet.res_skip_layers.3.weight_v', 'flow.flows.2.wavenet.in_layers.2.weight_g', 'posterior_encoder.wavenet.res_skip_layers.11.weight_g', 'flow.flows.3.wavenet.in_layers.3.weight_v', 'flow.flows.1.wavenet.res_skip_layers.3.weight_g', 'posterior_encoder.wavenet.res_skip_layers.13.weight_v', 'flow.flows.1.wavenet.in_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.7.weight_g', 'posterior_encoder.wavenet.in_layers.4.weight_g', 'flow.flows.3.wavenet.res_skip_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.10.weight_g', 'posterior_encoder.wavenet.res_skip_layers.4.weight_v', 'posterior_encoder.wavenet.res_skip_layers.14.weight_g', 'posterior_encoder.wavenet.res_skip_layers.11.weight_v', 'flow.flows.3.wavenet.in_layers.0.weight_g', 'posterior_encoder.wavenet.in_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.11.weight_g', 'flow.flows.3.wavenet.in_layers.2.weight_g', 'posterior_encoder.wavenet.res_skip_layers.3.weight_g', 'flow.flows.0.wavenet.in_layers.2.weight_v', 'flow.flows.0.wavenet.res_skip_layers.3.weight_g', 'flow.flows.2.wavenet.in_layers.1.weight_g', 'flow.flows.3.wavenet.in_layers.3.weight_g', 'flow.flows.2.wavenet.in_layers.1.weight_v', 'flow.flows.2.wavenet.in_layers.0.weight_v', 'flow.flows.1.wavenet.res_skip_layers.1.weight_v', 'posterior_encoder.wavenet.res_skip_layers.5.weight_v', 'posterior_encoder.wavenet.in_layers.12.weight_v', 'flow.flows.0.wavenet.res_skip_layers.0.weight_v', 'flow.flows.0.wavenet.in_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.5.weight_g', 'flow.flows.0.wavenet.res_skip_layers.1.weight_g', 'posterior_encoder.wavenet.in_layers.2.weight_v', 'flow.flows.0.wavenet.in_layers.3.weight_g', 'flow.flows.1.wavenet.res_skip_layers.1.weight_g']\n",
      "- This IS expected if you are initializing VitsModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VitsModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VitsModel were not initialized from the model checkpoint at facebook/mms-tts-tam and are newly initialized: ['flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”¹ Import all required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "from transformers import VitsModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# ðŸ”¹ Load model + tokenizer\n",
    "model = VitsModel.from_pretrained(\"facebook/mms-tts-tam\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mms-tts-tam\")\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "\n",
    "# ðŸ”¹ Load CSV\n",
    "df = pd.read_csv(\"hindi_to_tamil_results.csv\")\n",
    "\n",
    "\n",
    "tts_results = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tamil_text = row[\"tamil\"]\n",
    "    file_name = row[\"file\"].replace(\".wav\", \"_tamil_tts.wav\")\n",
    "    output_path = os.path.join(\"test_audio\", \"tamil_audio\", file_name)\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # ðŸ”¹ Generate speech\n",
    "    inputs = tokenizer(tamil_text, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        audio_waveform = model(**inputs).waveform\n",
    "\n",
    "    # ðŸ”¹ Save audio\n",
    "    sf.write(\n",
    "        output_path,\n",
    "        audio_waveform.squeeze().numpy(),\n",
    "        samplerate=model.config.sampling_rate\n",
    "    )\n",
    "\n",
    "    tts_results.append({\n",
    "        'file': row['file'],\n",
    "        'original_hindi': row['original'],\n",
    "        'translated_english': row['english'],\n",
    "        'translated_tamil': row['tamil'],\n",
    "        'tamil_audio_path': output_path\n",
    "    })\n",
    "\n",
    "\n",
    "# ðŸ”¹ Save results with audio paths\n",
    "tts_df = pd.DataFrame(tts_results)\n",
    "tts_df.to_csv(\"tamil_tts_results.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¬ Extracting audio track from Hindi video via ffmpeg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "ffmpeg version 8.0 Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with Apple clang version 17.0.0 (clang-1700.0.13.3)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/8.0_1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      60.  8.100 / 60.  8.100\n",
      "  libavcodec     62. 11.100 / 62. 11.100\n",
      "  libavformat    62.  3.100 / 62.  3.100\n",
      "  libavdevice    62.  1.100 / 62.  1.100\n",
      "  libavfilter    11.  4.100 / 11.  4.100\n",
      "  libswscale      9.  1.100 /  9.  1.100\n",
      "  libswresample   6.  1.100 /  6.  1.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'assets/hindi_demo_clip_long.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    title           : Pradhushan - http://www.archive.org/details/en_padaippu\n",
      "    encoder         : Lavf62.3.100\n",
      "    comment         : license:http://creativecommons.org/licenses/publicdomain/\n",
      "  Duration: 00:01:00.00, start: 0.000000, bitrate: 300 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 640x480, 167 kb/s, 12 fps, 12 tbr, 12288 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc62.11.100 libx264\n",
      "  Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'demo_outputs/hindi_demo_clip_long_audio.wav':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    INAM            : Pradhushan - http://www.archive.org/details/en_padaippu\n",
      "    ICMT            : license:http://creativecommons.org/licenses/publicdomain/\n",
      "    ISFT            : Lavf62.3.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc62.11.100 pcm_s16le\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "[out#0/wav @ 0x137627250] video:0KiB audio:1875KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.010833%\n",
      "size=    1875KiB time=00:01:00.00 bitrate= 256.0kbits/s speed=1.93e+03x elapsed=0:00:00.03    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—£ï¸ Running Hindi ASR...\n",
      "   Detected Hindi text: à¤†à¤¨à¤¿à¤¯à¤¾ à¤®à¥à¤–à¤¯à¤¾à¤šà¤œà¥à¤¯à¤¾ à¤µà¤¿à¤•à¤¾ à¥€ à¤—à¥à¤¨à¥€ à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤µà¤¨ à¤•à¥‡ à¤¬à¤ª à¤‰à¤ªà¤° à¤¤à¥‡à¤¶à¥‹à¤¾à¤•à¤£ à¤¸à¤¬à¤­à¤¨à¥‡ à¤¨à¤¾à¤¹à¤¸ à¤•à¤¾ à¤†à¤œ à¤®à¥ˆ à¤ªà¥à¤°à¤¦à¥‚à¤·à¤¿à¤£ à¤•à¥‡ à¤•à¤¾à¤°à¤£ à¤” à¤¨à¤°à¤µà¤¾à¤°à¤£ à¤‡à¤¸ à¤µà¤¿à¤·à¤¯ à¤ªà¤° à¤…à¤ªà¤¨à¥‡ à¤µà¤¿à¤šà¤¾à¤° à¤•à¥à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥€ à¤¹à¥‹à¤‚ à¤ªà¥à¤°à¤¾à¤¨à¥‡ à¤¸à¤®à¤¾à¤¨à¥‡ à¤®à¥‡à¤‚ à¤¹ à¤¾ à¤®à¥‡à¤‚ à¤ªà¤šà¤¾ à¤¸à¤¾ à¤— à¤¹à¥‹à¤¤à¥‡ à¤¤à¥€à¤¨à¤šà¤¾à¤¸à¥Œ à¤²à¥‹à¤— à¤—à¤¾à¤¯à¤µà¥‡ à¤¬à¤•à¤°à¥‡ à¤¹à¥à¤¤à¥‡ à¤®à¥à¤°à¥à¤—à¥€ à¤®à¤¯à¤¨à¤¾ à¤•à¤¹à¥‹à¤¤à¤° à¤šà¤° à¤”à¤° à¤˜à¤°à¥‡ à¤¬à¤°à¥‡ à¤•à¥‡ à¤¤ à¤ªà¥Œà¤§à¥‡ à¤®à¤¦à¥€à¤®à¤¾à¤²à¥‡ à¤‡à¤¸ à¤¤à¤°à¤¬ à¤œà¥€ à¤¬à¤¿à¤‚à¤¤à¥ à¤…à¤ªà¤¨à¤¸ à¤•à¤¾ à¤•à¥à¥‡ à¤®à¥‡à¤‚ à¤¸à¤‚à¤¤à¥à¤²à¤¨ à¤¥à¤¾ à¤–à¥‡à¤¤à¥‹à¤‚ à¤®à¥‡à¤‚ à¤«à¤¸à¤² à¤…à¤šà¥à¤›à¤¾ à¤¹à¥‹à¤¤à¤¾ à¤•à¤¾ à¤œà¥€à¤µà¤¨ à¤¸à¥à¤– à¤®à¥‡ à¤¥à¤¾ à¤²à¥‡à¤•à¤¿à¤¨ à¤†à¤ª à¤•à¤¨ à¤¨à¥à¤¯à¤¾ à¤¬à¤¹à¥à¤¤\n",
      "ðŸŒ Translating Hindi -> English -> Tamil...\n",
      "   English: Three hundred and a hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred and one hundred.\n",
      "   Tamil  : à®…à®¨à®¿à®¯à®¾ à®®à¯à®•à¯à®¯à®šà¯à®œà¯à®¯ à®µà®¿à®•à®¾à®¸à¯ à®•à¯à®©à®¿ à®šà®¿à®•à¯à®·à®¾ à®ªà®¾à®ªà¯ à®¤à¯‡à®·à¯‹à®•à®©à¯ à®®à¯€à®¤à¯ à®•à®¾à®Ÿà¯ à®…à®´à®¿à®¨à¯à®¤à¯ à®®à®¾à®šà¯à®ªà®¾à®Ÿà¯à®Ÿà®¾à®²à¯ à®Žà®²à¯à®²à®¾à®®à¯‡ à®‡à®©à¯à®±à¯ à®‡à®¨à¯à®¤ à®¤à®²à¯ˆà®ªà¯à®ªà¯ˆà®ªà¯ à®ªà®±à¯à®±à®¿à®¯à¯à®®à¯ à®…à®¤à®©à¯ à®¤à®Ÿà¯à®ªà¯à®ªà¯ à®ªà®±à¯à®±à®¿à®¯à¯à®®à¯ à®Žà®©à®¤à¯ à®•à®°à¯à®¤à¯à®¤à¯à®•à¯à®•à®³à¯ˆà®ªà¯ à®ªà®•à®¿à®°à¯à®¨à¯à®¤à¯ à®•à¯Šà®³à¯à®³ à®µà®¿à®°à¯à®®à¯à®ªà¯à®•à®¿à®±à¯‡à®©à¯, à®ªà®´à®™à¯à®•à®¾à®²à®¤à¯à®¤à®¿à®²à¯ à®®à¯à®¨à¯à®¨à¯‚à®±à¯à®±à¯ à®¨à®¾à®±à¯à®ªà®¤à¯ à®ªà¯‡à®°à¯, à®®à®¾à®Ÿà¯à®•à®³à¯, à®†à®Ÿà¯à®•à®³à¯, à®•à¯‹à®´à®¿à®•à®³à¯, à®®à®¾à®Ÿà¯à®•à®³à¯, à®•à®¾à®²à¯à®¨à®Ÿà¯ˆà®•à®³à¯, à®µà¯€à®Ÿà¯à®Ÿà¯ˆà®šà¯ à®šà¯à®±à¯à®±à®¿ à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯ à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à®°à®™à¯à®•à®³à¯, à®®à¯à®¤à®²à®¿à®¯à®© à®…à®±à¯à®µà®Ÿà¯ˆ à®¨à®©à¯à®±à®¾à®• à®‡à®°à¯à®¨à¯à®¤à®¿à®°à¯à®•à¯à®•à¯à®®à¯, à®µà®¾à®´à¯à®•à¯à®•à¯ˆ à®®à®•à®¿à®´à¯à®šà¯à®šà®¿à®¯à®¾à®• à®‡à®°à¯à®¨à¯à®¤à®¤à¯, à®†à®©à®¾à®²à¯ à®¨à¯€à®™à¯à®•à®³à¯ à®®à®©à¯ˆà®µà®¿ à®®à®¿à®•à®µà¯à®®à¯ à®®à®•à®¿à®´à¯à®šà¯à®šà®¿à®¯à®±à¯à®±à®µà®°à¯.\n",
      "ðŸ”Š Generating Tamil speech with Svara TTS v1...\n",
      "   Attempting to connect to Svara TTS API...\n",
      "   âš ï¸ API connection failed: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /v1/text-to-speech (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x3104de5d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "   Falling back to gTTS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "ffmpeg version 8.0 Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with Apple clang version 17.0.0 (clang-1700.0.13.3)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/8.0_1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      60.  8.100 / 60.  8.100\n",
      "  libavcodec     62. 11.100 / 62. 11.100\n",
      "  libavformat    62.  3.100 / 62.  3.100\n",
      "  libavdevice    62.  1.100 / 62.  1.100\n",
      "  libavfilter    11.  4.100 / 11.  4.100\n",
      "  libswscale      9.  1.100 /  9.  1.100\n",
      "  libswresample   6.  1.100 /  6.  1.100\n",
      "[mp3 @ 0x128704650] Estimating duration from bitrate, this may be inaccurate\n",
      "Input #0, mp3, from 'demo_outputs/hindi_demo_clip_long_gtts.mp3':\n",
      "  Duration: 00:01:07.82, start: 0.000000, bitrate: 64 kb/s\n",
      "  Stream #0:0: Audio: mp3 (mp3float), 24000 Hz, mono, fltp, 64 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'demo_outputs/hindi_demo_clip_long_gtts.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf62.3.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc62.11.100 pcm_s16le\n",
      "[out#0/wav @ 0x129104320] video:0KiB audio:2120KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.003594%\n",
      "size=    2120KiB time=00:01:07.82 bitrate= 256.0kbits/s speed=2.65e+03x elapsed=0:00:00.02    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽžï¸ Replacing video audio track with Tamil dub via ffmpeg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "ffmpeg version 8.0 Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with Apple clang version 17.0.0 (clang-1700.0.13.3)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/8.0_1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      60.  8.100 / 60.  8.100\n",
      "  libavcodec     62. 11.100 / 62. 11.100\n",
      "  libavformat    62.  3.100 / 62.  3.100\n",
      "  libavdevice    62.  1.100 / 62.  1.100\n",
      "  libavfilter    11.  4.100 / 11.  4.100\n",
      "  libswscale      9.  1.100 /  9.  1.100\n",
      "  libswresample   6.  1.100 /  6.  1.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'assets/hindi_demo_clip_long.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    title           : Pradhushan - http://www.archive.org/details/en_padaippu\n",
      "    encoder         : Lavf62.3.100\n",
      "    comment         : license:http://creativecommons.org/licenses/publicdomain/\n",
      "  Duration: 00:01:00.00, start: 0.000000, bitrate: 300 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 640x480, 167 kb/s, 12 fps, 12 tbr, 12288 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc62.11.100 libx264\n",
      "  Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "[aist#1:0/pcm_s16le @ 0x124608e90] Guessed Channel Layout: mono\n",
      "Input #1, wav, from 'demo_outputs/hindi_demo_clip_long_tamil.wav':\n",
      "  Duration: 00:01:00.00, bitrate: 256 kb/s\n",
      "  Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (copy)\n",
      "  Stream #1:0 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp4, to 'demo_outputs/hindi_demo_clip_long_tamil_dub.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    title           : Pradhushan - http://www.archive.org/details/en_padaippu\n",
      "    comment         : license:http://creativecommons.org/licenses/publicdomain/\n",
      "    encoder         : Lavf62.3.100\n",
      "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 640x480, q=2-31, 167 kb/s, 12 fps, 12 tbr, 12288 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc62.11.100 libx264\n",
      "  Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 16000 Hz, mono, fltp, 69 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc62.11.100 aac\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tamil dubbed video ready! Saved to demo_outputs/hindi_demo_clip_long_tamil_dub.mp4\n",
      "âœ… Metadata saved to demo_outputs/hindi_demo_clip_long_metadata.json\n",
      "âœ… TTS method used: gTTS (fallback)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[out#0/mp4 @ 0x1246060d0] video:1226KiB audio:377KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 1.468320%\n",
      "frame=  720 fps=0.0 q=-1.0 Lsize=    1627KiB time=00:00:59.83 bitrate= 222.7kbits/s speed= 259x elapsed=0:00:00.23    \n",
      "[aac @ 0x12460a720] Qavg: 63460.449\n"
     ]
    }
   ],
   "source": [
    "# Hindi video -> Tamil dubbed video pipeline (using Svara TTS v1)\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import requests\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC,\n",
    "    Wav2Vec2Processor,\n",
    "    M2M100ForConditionalGeneration,\n",
    "    M2M100Tokenizer,\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "video_path = Path(\"assets/hindi_demo_clip_long.mp4\")\n",
    "output_dir = Path(\"demo_outputs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "clip_tag = video_path.stem\n",
    "\n",
    "raw_audio_path = output_dir / f\"{clip_tag}_audio.wav\"\n",
    "tamil_audio_path = output_dir / f\"{clip_tag}_tamil.wav\"\n",
    "dubbed_video_path = output_dir / f\"{clip_tag}_tamil_dub.mp4\"\n",
    "metadata_path = output_dir / f\"{clip_tag}_metadata.json\"\n",
    "\n",
    "manual_offset_sec = 11.0\n",
    "\n",
    "# Svara TTS API endpoint (local deployment)\n",
    "SVARA_API_URL = \"http://localhost:8080/v1/text-to-speech\"\n",
    "\n",
    "if not video_path.exists():\n",
    "    raise FileNotFoundError(f\"Input video not found at {video_path}\")\n",
    "\n",
    "print(\"ðŸŽ¬ Extracting audio track from Hindi video via ffmpeg...\")\n",
    "subprocess.run([\n",
    "    \"ffmpeg\", \"-y\", \"-i\", str(video_path),\n",
    "    \"-ac\", \"1\", \"-ar\", \"16000\", \"-vn\", str(raw_audio_path)\n",
    "], check=True)\n",
    "\n",
    "target_sr = 16000\n",
    "waveform, sample_rate = torchaudio.load(str(raw_audio_path))\n",
    "if sample_rate != target_sr:\n",
    "    waveform = torchaudio.transforms.Resample(sample_rate, target_sr)(waveform)\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)\n",
    "waveform_np = waveform.squeeze().numpy()\n",
    "target_length = waveform_np.shape[-1]\n",
    "\n",
    "# Load Hindi ASR model\n",
    "if \"stt_processor\" not in globals():\n",
    "    print(\"ðŸ“¥ Loading Hindi ASR model...\")\n",
    "    stt_processor = Wav2Vec2Processor.from_pretrained(\"ai4bharat/indicwav2vec-hindi\")\n",
    "if \"stt_model\" not in globals():\n",
    "    stt_model = Wav2Vec2ForCTC.from_pretrained(\"ai4bharat/indicwav2vec-hindi\")\n",
    "    stt_model.eval()\n",
    "\n",
    "print(\"ðŸ—£ï¸ Running Hindi ASR...\")\n",
    "with torch.no_grad():\n",
    "    input_values = stt_processor(waveform_np, sampling_rate=target_sr, return_tensors=\"pt\").input_values\n",
    "    logits = stt_model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "hindi_text = stt_processor.decode(predicted_ids[0])\n",
    "print(f\"   Detected Hindi text: {hindi_text}\")\n",
    "\n",
    "# Load translation model\n",
    "if \"translation_model\" not in globals():\n",
    "    print(\"ðŸ“¥ Loading translation model...\")\n",
    "    translation_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "    translation_model.eval()\n",
    "if \"translation_tokenizer\" not in globals():\n",
    "    translation_tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "def translate_text(text: str, src_lang: str, tgt_lang: str) -> str:\n",
    "    translation_tokenizer.src_lang = src_lang\n",
    "    encoded = translation_tokenizer(text, return_tensors=\"pt\")\n",
    "    generated_tokens = translation_model.generate(\n",
    "        **encoded, forced_bos_token_id=translation_tokenizer.get_lang_id(tgt_lang)\n",
    "    )\n",
    "    return translation_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"ðŸŒ Translating Hindi -> English -> Tamil...\")\n",
    "english_text = translate_text(hindi_text, \"hi\", \"en\")\n",
    "tamil_text = translate_text(english_text, \"en\", \"ta\")\n",
    "\n",
    "# Manual override from metadata JSON\n",
    "if metadata_path.exists():\n",
    "    manual_meta = json.loads(metadata_path.read_text(encoding=\"utf-8\"))\n",
    "    english_text = manual_meta.get(\"english_translation\", english_text)\n",
    "    tamil_text = manual_meta.get(\"tamil_translation\", tamil_text)\n",
    "\n",
    "print(f\"   English: {english_text}\")\n",
    "print(f\"   Tamil  : {tamil_text}\")\n",
    "\n",
    "print(\"ðŸ”Š Generating Tamil speech with Svara TTS v1...\")\n",
    "\n",
    "# Using Svara TTS API\n",
    "try:\n",
    "    print(\"   Attempting to connect to Svara TTS API...\")\n",
    "    response = requests.post(\n",
    "        SVARA_API_URL,\n",
    "        json={\n",
    "            \"text\": tamil_text,\n",
    "            \"voice_id\": \"ta_female\",  # Tamil female voice\n",
    "            \"stream\": True\n",
    "        },\n",
    "        stream=True,\n",
    "        timeout=60\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Save PCM audio\n",
    "        pcm_path = output_dir / f\"{clip_tag}_tamil_temp.pcm\"\n",
    "        with open(pcm_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        # Convert PCM to WAV (Svara outputs at 24kHz)\n",
    "        subprocess.run([\n",
    "            \"ffmpeg\", \"-y\", \"-f\", \"s16le\", \"-ar\", \"24000\", \"-ac\", \"1\",\n",
    "            \"-i\", str(pcm_path), \"-ar\", str(target_sr), str(tamil_audio_path)\n",
    "        ], check=True)\n",
    "        \n",
    "        pcm_path.unlink()\n",
    "        \n",
    "        # Load the generated audio\n",
    "        tamil_waveform, tamil_sr = torchaudio.load(str(tamil_audio_path))\n",
    "        if tamil_sr != target_sr:\n",
    "            tamil_waveform = torchaudio.transforms.Resample(tamil_sr, target_sr)(tamil_waveform)\n",
    "        tamil_waveform_np = tamil_waveform.squeeze().numpy()\n",
    "        \n",
    "        print(\"   âœ“ Successfully generated audio via Svara TTS API\")\n",
    "        tts_method = \"Svara TTS v1 API\"\n",
    "        \n",
    "    else:\n",
    "        raise Exception(f\"API returned status code {response.status_code}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ API connection failed: {e}\")\n",
    "    print(\"   Falling back to gTTS...\")\n",
    "    \n",
    "    from gtts import gTTS\n",
    "    gtts_mp3 = output_dir / f\"{clip_tag}_gtts.mp3\"\n",
    "    gtts_wav = output_dir / f\"{clip_tag}_gtts.wav\"\n",
    "    \n",
    "    gtts = gTTS(text=tamil_text, lang=\"ta\")\n",
    "    gtts.save(gtts_mp3)\n",
    "    \n",
    "    subprocess.run([\n",
    "        \"ffmpeg\", \"-y\", \"-i\", str(gtts_mp3),\n",
    "        \"-ac\", \"1\", \"-ar\", str(target_sr), str(gtts_wav)\n",
    "    ], check=True)\n",
    "    \n",
    "    tamil_waveform, tamil_sr = torchaudio.load(str(gtts_wav))\n",
    "    if tamil_sr != target_sr:\n",
    "        tamil_waveform = torchaudio.transforms.Resample(tamil_sr, target_sr)(tamil_waveform)\n",
    "    tamil_waveform_np = tamil_waveform.squeeze().numpy()\n",
    "    \n",
    "    sf.write(str(tamil_audio_path), tamil_waveform_np, samplerate=target_sr)\n",
    "    tts_method = \"gTTS (fallback)\"\n",
    "    \n",
    "    gtts_mp3.unlink(missing_ok=True)\n",
    "    gtts_wav.unlink(missing_ok=True)\n",
    "\n",
    "# Align audio with video\n",
    "aligned_waveform = np.zeros(target_length, dtype=np.float32)\n",
    "offset_samples = min(int(manual_offset_sec * target_sr), target_length - 1)\n",
    "trimmed_length = min(target_length - offset_samples, len(tamil_waveform_np))\n",
    "aligned_waveform[offset_samples:offset_samples + trimmed_length] = tamil_waveform_np[:trimmed_length]\n",
    "\n",
    "sf.write(str(tamil_audio_path), aligned_waveform, samplerate=target_sr)\n",
    "\n",
    "print(\"ðŸŽžï¸ Replacing video audio track with Tamil dub via ffmpeg...\")\n",
    "subprocess.run([\n",
    "    \"ffmpeg\", \"-y\", \"-i\", str(video_path), \"-i\", str(tamil_audio_path),\n",
    "    \"-c:v\", \"copy\", \"-map\", \"0:v:0\", \"-map\", \"1:a:0\", str(dubbed_video_path)\n",
    "], check=True)\n",
    "\n",
    "metadata = {\n",
    "    \"source_video\": str(video_path),\n",
    "    \"hindi_transcript\": hindi_text,\n",
    "    \"english_translation\": english_text,\n",
    "    \"tamil_translation\": tamil_text,\n",
    "    \"tamil_audio\": str(tamil_audio_path),\n",
    "    \"dubbed_video\": str(dubbed_video_path),\n",
    "    \"start_offset_sec\": manual_offset_sec,\n",
    "    \"tts_engine\": tts_method,\n",
    "    \"model_name\": \"kenpath/svara-tts-v1\",\n",
    "    \"voice_id\": \"ta_female\",\n",
    "}\n",
    "metadata_path.write_text(json.dumps(metadata, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"âœ… Tamil dubbed video ready! Saved to\", dubbed_video_path)\n",
    "print(\"âœ… Metadata saved to\", metadata_path)\n",
    "print(f\"âœ… TTS method used: {tts_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
